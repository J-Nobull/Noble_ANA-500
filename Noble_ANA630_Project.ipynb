{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d793bbe-9ae4-493e-87a9-cd3613a55070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages are available!\n"
     ]
    }
   ],
   "source": [
    "    # Load packages for (ADV_analytics kernel)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"All packages are available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd86a4-6047-4946-89f0-bd3a3fef9d49",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1.  1 blank 'party_detailed' vote cast for Donald TRUMP, corrected party to REPUBLICAN pior to import.  \n",
    "\n",
    "2.  Remaining blanks verified as writein votes cast for \"OTHER\" party candidates.\n",
    "\n",
    "3.  94 samples labeled \"DEMOCRAT/REPUBLICAN\" in 'party_detailed':  \n",
    "    utilized PA.gov to confirm HEFFLEY = REPUBLICAN, LONGIETTI = DEMOCRAT.\n",
    "\n",
    "4.  In Nov 2020, there were four recognized political parties in Pennsylvania.  \n",
    "    Democratic Party, Green Party, Libertarian Party, Republican Party.  \n",
    "    DEM and REP votes accounted for 96% of total votes; \"OTHER\" will combine any vote for another.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ab0e60-e165-4489-8946-d08898b7abf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Import the first dataset\n",
    "VTD_import = pd.read_csv('PA_l2_turnout_stats_block20.csv')\n",
    "#print(VTD_import.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107784dc-f7c5-43bb-99d6-168f648a8cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Inspect VTD file\n",
    "    # Retain only GEOID and 03NOV2020 columns\n",
    "vtd_columns = ['geoid20',\n",
    "    'g20201103_reg_all', 'g20201103_voted_all', 'g20201103_pct_voted_all',\n",
    "    'g20201103_reg_eur', 'g20201103_voted_eur', 'g20201103_pct_voted_eur',\n",
    "    'g20201103_reg_hisp', 'g20201103_voted_hisp', 'g20201103_pct_voted_hisp',\n",
    "    'g20201103_reg_aa', 'g20201103_voted_aa', 'g20201103_pct_voted_aa',\n",
    "    'g20201103_reg_esa', 'g20201103_voted_esa', 'g20201103_pct_voted_esa',\n",
    "    'g20201103_reg_oth', 'g20201103_voted_oth', 'g20201103_pct_voted_oth',\n",
    "    'g20201103_reg_unk', 'g20201103_voted_unk', 'g20201103_pct_voted_unk']\n",
    "\n",
    "    # Change VAR names\n",
    "VTD_inspect = (VTD_import[vtd_columns]\n",
    "               .rename(columns=lambda x: x.replace('g20201103_', ''))\n",
    "               .rename(columns=lambda x: x.replace('pct_voted', 'turnout'))\n",
    "               .rename(columns=lambda x: x.replace('_esa', '_asia'))\n",
    "               .copy())\n",
    "\n",
    "    # Create 'GEOID' column and truncate, drop original\n",
    "VTD_inspect['GEOID'] = VTD_inspect['geoid20'].astype(str).str[:11]\n",
    "VTD_inspect = VTD_inspect.drop(columns=['geoid20'])\n",
    "                               \n",
    "    # Confirm\n",
    "#print(VTD_inspect.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2a39d6-ed9b-473c-8064-8740230ef132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Define aggregation rules\n",
    "    # Use sum for all columns that do not contain 'turnout' (excluding GEOID)\n",
    "sum_cols = [col for col in VTD_inspect.columns if 'turnout' not in col and col != 'GEOID']\n",
    "    # Use mean for columns containing 'turnout'\n",
    "mean_cols = [col for col in VTD_inspect.columns if 'turnout' in col]\n",
    "\n",
    "    # Group by GEOID and aggregate\n",
    "VTD_transform = VTD_inspect.groupby('GEOID').agg({\n",
    "    **{col: 'sum' for col in sum_cols}, \n",
    "    **{col: 'mean' for col in mean_cols}}).reset_index()\n",
    "\n",
    "    # Convert 'reg_' and 'voted_' columns to int32\n",
    "convert_cols = [col for col in VTD_transform.columns if \n",
    "                col.startswith('reg_') or col.startswith('voted_')]\n",
    "VTD_transform[convert_cols] = VTD_transform[convert_cols].astype('int32')\n",
    "\n",
    "   # Confirm\n",
    "#print(VTD_transform.info())\n",
    "#print(VTD_transform['GEOID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "314ee1f1-255d-4679-9f53-11e4d0a7996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Numbers: [14 97 79 25 25  4 77 90 83 56]\n",
      "Mean: 55.00\n",
      "Median: 66.50\n",
      "Standard Deviation: 33.04\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "\n",
    "# Generate 10 random integers between 1 and 100\n",
    "random_numbers = np.random.randint(1, 101, size=10)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_value = np.mean(random_numbers)\n",
    "median_value = np.median(random_numbers)\n",
    "std_dev_value = np.std(random_numbers)\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Numbers: {random_numbers}\")\n",
    "print(f\"Mean: {mean_value:.2f}\")\n",
    "print(f\"Median: {median_value:.2f}\")\n",
    "print(f\"Standard Deviation: {std_dev_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c8109529-4149-48a7-baf0-b3b90f5e397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3446 entries, 0 to 3445\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   GEOID         3446 non-null   object \n",
      " 1   reg_all       3446 non-null   int32  \n",
      " 2   voted_all     3446 non-null   int32  \n",
      " 3   reg_eur       3446 non-null   int32  \n",
      " 4   voted_eur     3446 non-null   int32  \n",
      " 5   reg_hisp      3446 non-null   int32  \n",
      " 6   voted_hisp    3446 non-null   int32  \n",
      " 7   reg_aa        3446 non-null   int32  \n",
      " 8   voted_aa      3446 non-null   int32  \n",
      " 9   reg_asia      3446 non-null   int32  \n",
      " 10  voted_asia    3446 non-null   int32  \n",
      " 11  reg_oth       3446 non-null   int32  \n",
      " 12  voted_oth     3446 non-null   int32  \n",
      " 13  reg_unk       3446 non-null   int32  \n",
      " 14  voted_unk     3446 non-null   int32  \n",
      " 15  turnout_all   3446 non-null   float64\n",
      " 16  turnout_eur   3446 non-null   float64\n",
      " 17  turnout_hisp  3446 non-null   float64\n",
      " 18  turnout_aa    3446 non-null   float64\n",
      " 19  turnout_asia  3446 non-null   float64\n",
      " 20  turnout_oth   3446 non-null   float64\n",
      " 21  turnout_unk   3446 non-null   float64\n",
      "dtypes: float64(7), int32(14), object(1)\n",
      "memory usage: 404.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(VTD_transform.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71196c55-8605-477f-85e0-bc7cf0d71f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['turnout_gap_eur', 'turnout_gap_hisp', 'turnout_gap_aa', 'turnout_gap_asia', 'turnout_gap_oth', 'turnout_gap_unk'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     final_cols\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturnout_gap_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Reorder\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m VTD_tidy \u001b[38;5;241m=\u001b[39m \u001b[43mVTD_transform\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfinal_cols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m VTD_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEOID-5\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m VTD_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEOID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m5\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['turnout_gap_eur', 'turnout_gap_hisp', 'turnout_gap_aa', 'turnout_gap_asia', 'turnout_gap_oth', 'turnout_gap_unk'] not in index\""
     ]
    }
   ],
   "source": [
    "    # Create \"turnout_gap\" for each group (turnout_grp - turnout_all)\n",
    "base_cols = ['all', 'eur', 'hisp', 'aa', 'asia', 'oth', 'unk']\n",
    "for grp in base_cols:\n",
    "   col_name = f'turnout_{grp}'\n",
    "   VTD_transform[col_name] = (VTD_transform[col_name] * 100).round(2)\n",
    "for grp in base_cols[1:]:\n",
    "   VTD_transform[f'turnout_{grp}'] = VTD_transform[\n",
    "   f'turnout_{grp}'] - VTD_transform['turnout_all']\n",
    "\n",
    "    # Define column order, No turnout_gap for 'all'\n",
    "final_cols = ['GEOID']\n",
    "for grp in base_cols:\n",
    "   final_cols.extend([f'reg_{grp}', f'voted_{grp}', f'turnout_{grp}'])\n",
    "   if grp != 'all':\n",
    "        final_cols.append(f'turnout_gap_{grp}')\n",
    "\n",
    "    # Reorder\n",
    "VTD_tidy = VTD_transform[final_cols]\n",
    "VTD_tidy['GEOID-5'] = VTD_tidy['GEOID'].astype(str).str[:5]\n",
    "\n",
    "    # Confirm final tidy dataframe\n",
    "#print(VTD_tidy.info())\n",
    "#print(VTD_tidy.head())\n",
    "#VTD_tidy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e7769-047f-4533-b592-498331020f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VTD_tidy.to_excel('VTD_tidy.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14cb0bba-67ad-4934-8d47-22b98bd71f47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Import and tidy DHC dataset\n",
    "DHC_import = pd.read_csv('pa_dhc_2020_t.csv')\n",
    "#print(DHC_import.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4d05e06-0624-410a-b39f-80c8041ddc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Format GEOID: rename and truncate\n",
    "DHC_inspect = DHC_import.rename(columns={'GEOID20': 'GEOID'})\n",
    "DHC_inspect['GEOID'] = DHC_inspect['GEOID'].astype(str).str[:11]\n",
    "\n",
    "    # Drop columns with constant entries or conflicts (County = COUNTYFP20 = GEOID)\n",
    "DHC_inspect = DHC_inspect.drop(columns=[\n",
    "    'STATEFP20', 'STATE', 'COUNTYFP20', 'COUNTY', 'NOT_DEF'])\n",
    "\n",
    "    # Confirm\n",
    "#print(DHC_inspect.info())\n",
    "#print(DHC_inspect['GEOID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e35e737f-4d02-494c-9d16-4cf54a5822ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Remove any record where total pop = 0, to prevent NAs in ratios\n",
    "DHC_transform = DHC_inspect[(DHC_inspect['TOT_POP'] != 0) & (\n",
    "                            DHC_inspect['HH_IN_HH'] != 0)].copy()\n",
    "\n",
    "    # Compute new ratios for basic population metrics\n",
    "DHC_transform['URBAN_RATIO']  = (DHC_transform['URBAN'] / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['RURAL_RATIO']  = (DHC_transform['RURAL'] / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['MALE_RATIO']   = (DHC_transform['MALE'] / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['FEMALE_RATIO'] = (DHC_transform['FEMALE'] / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "\n",
    "    # Compute new age groups\n",
    "DHC_transform['F_U18']    = DHC_transform['U5_F'] + DHC_transform['5_9_F'] + \\\n",
    "        DHC_transform['10_14_F'] + DHC_transform['15_17_F']\n",
    "DHC_transform['F_18-20s'] = DHC_transform['18_19_F'] + DHC_transform['20_F']\n",
    "DHC_transform['F_30s']    = DHC_transform['30_34_F'] + DHC_transform['35_39_F']\n",
    "DHC_transform['F_40s']    = DHC_transform['40_44_F'] + DHC_transform['45_49_F']\n",
    "DHC_transform['F_50s']    = DHC_transform['50_54_F'] + DHC_transform['55_59_F']\n",
    "DHC_transform['F_60up']    = (DHC_transform['60_61_F'] + DHC_transform['62_64_F'] + \\\n",
    "        DHC_transform['65_66_F'] + DHC_transform['67_69_F'] + DHC_transform['70_74_F'] + \\\n",
    "        DHC_transform['75_79_F'] + DHC_transform['80_84_F'] + DHC_transform['85_O_F'])\n",
    "DHC_transform['M_U18']    = DHC_transform['U5_M'] + DHC_transform['5_9_M'] + \\\n",
    "        DHC_transform['10_14_M'] + DHC_transform['15_17_M']\n",
    "DHC_transform['M_18-20s'] = DHC_transform['18_19_M'] + DHC_transform['20_M']\n",
    "DHC_transform['M_30s']    = DHC_transform['30_34_M'] + DHC_transform['35_39_M']\n",
    "DHC_transform['M_40s']    = DHC_transform['40_44_M'] + DHC_transform['45_49_M']\n",
    "DHC_transform['M_50s']    = DHC_transform['50_54_M'] + DHC_transform['55_59_M']\n",
    "DHC_transform['M_60up']    = (DHC_transform['60_61_M'] + DHC_transform['62_64_M'] + \\\n",
    "        DHC_transform['65_66_M'] + DHC_transform['67_69_M'] + DHC_transform['70_74_M'] + \\\n",
    "        DHC_transform['75_79_M'] + DHC_transform['80_84_M'] + DHC_transform['85_O_M'])\n",
    "\n",
    "    # Compute ratios for the new age groups (relative to total FEMALE or MALE)\n",
    "DHC_transform['F_U18_RATIO']    = (DHC_transform['F_U18']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['F_18-20s_RATIO'] = (DHC_transform['F_18-20s'] / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['F_30s_RATIO']    = (DHC_transform['F_30s']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['F_40s_RATIO']    = (DHC_transform['F_40s']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['F_50s_RATIO']    = (DHC_transform['F_50s']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['F_60up_RATIO']    = (DHC_transform['F_60up']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['M_U18_RATIO']    = (DHC_transform['M_U18']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['M_18-20s_RATIO'] = (DHC_transform['M_18-20s'] / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['M_30s_RATIO']    = (DHC_transform['M_30s']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['M_40s_RATIO']    = (DHC_transform['M_40s']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['M_50s_RATIO']    = (DHC_transform['M_50s']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "DHC_transform['M_60up_RATIO']    = (DHC_transform['M_60up']    / DHC_transform['TOT_POP'] * 100).round(2)\n",
    "\n",
    "    # Define new household groups sexuality will be considered\n",
    "    # Marital status not considered, assuming relationship / marriage will have same outcome\n",
    "DHC_transform['O_SEX_REL'] = DHC_transform['O_SEX_MAR'] + DHC_transform['O_SEX_UMAR']\n",
    "DHC_transform['S_SEX_REL'] = DHC_transform['S_SEX_MAR'] + DHC_transform['S_SEX_UMAR']\n",
    "    # \"OTHER\" = non-related (roommates) and persons in institution quarters\n",
    "DHC_transform['OTHER_HH'] = DHC_transform['OTH_NONREL'] + DHC_transform['GRP_QUART']\n",
    "\n",
    "    # --- Compute Ratios for housing groups\n",
    "DHC_transform['F_SINGLE_RATIO'] = (DHC_transform['F_ALONE'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "DHC_transform['M_SINGLE_RATIO'] = (DHC_transform['M_ALONE'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "DHC_transform['F_LED_HH_RATIO'] = (DHC_transform['F_N_ALONE'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "DHC_transform['M_LED_HH_RATIO'] = (DHC_transform['M_N_ALONE'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "DHC_transform['OTH_HH_RATIO'] = (DHC_transform['OTHER_HH'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "DHC_transform['OPPSEX_RATIO'] = (DHC_transform['O_SEX_REL'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "DHC_transform['SAMESEX_RATIO'] = (DHC_transform['S_SEX_REL'] / DHC_transform['HH_IN_HH'] * 100).round(2)\n",
    "\n",
    "    # Confirm\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#print(DHC_transform.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7ece42a-1dd9-42f8-b0c5-cb52bab7c269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Define final column order\n",
    "final_cols = [\n",
    "    'GEOID', 'TOT_POP', \n",
    "    'URBAN', 'URBAN_RATIO', \n",
    "    'RURAL', 'RURAL_RATIO', \n",
    "    'MED_AGE', 'MED_AGE_M', 'MED_AGE_F', \n",
    "    'F_U18', 'F_U18_RATIO', 'M_U18', 'M_U18_RATIO',  \n",
    "    'F_18-20s', 'F_18-20s_RATIO', 'M_18-20s', 'M_18-20s_RATIO',  \n",
    "    'F_30s', 'F_30s_RATIO', 'M_30s', 'M_30s_RATIO',  \n",
    "    'F_40s', 'F_40s_RATIO', 'M_40s', 'M_40s_RATIO',  \n",
    "    'F_50s', 'F_50s_RATIO', 'M_50s', 'M_50s_RATIO',  \n",
    "    'F_60up', 'F_60up_RATIO', 'M_60up', 'M_60up_RATIO',  \n",
    "    'MALE', 'MALE_RATIO', 'FEMALE', 'FEMALE_RATIO',  \n",
    "    'F_ALONE', 'F_SINGLE_RATIO', 'M_ALONE', 'M_SINGLE_RATIO', \n",
    "    'F_N_ALONE', 'F_LED_HH_RATIO', 'M_N_ALONE', 'M_LED_HH_RATIO',\n",
    "    'O_SEX_REL', 'OPPSEX_RATIO', 'S_SEX_REL', 'SAMESEX_RATIO', \n",
    "    'OTHER_HH', 'OTH_HH_RATIO', 'HH_IN_HH']\n",
    "DHC_tidy = DHC_transform[final_cols]\n",
    "\n",
    "    # Confirm\n",
    "#print(DHC_tidy.info())\n",
    "#DHC_tidy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c26101d-493b-4562-b987-f9bdc233820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DHC_tidy.to_excel('DHC_tidy.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e733d7bc-7187-49f7-a3bd-03ea2bfc9604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Import third dataset\n",
    "MEDSL_import = pd.read_csv('pa_medsl_20_general.csv')\n",
    "\n",
    "    # Inspect\n",
    "#print(MEDSL_import.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff0d2dbe-1a66-4757-af05-8ef1e0ae0f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Inspect MEDSL file\n",
    "    # Retain only the needed columns\n",
    "MEDSL_inspect = MEDSL_import[['office', 'county_fips', 'precinct',  \n",
    "                            'party_detailed', 'votes']]\n",
    "\n",
    "    # Rename columns\n",
    "MEDSL_inspect = MEDSL_inspect.rename(columns={\n",
    "    'county_fips': 'GEOID-5', \n",
    "    'party_detailed': 'PARTY', \n",
    "    'votes': 'VOTES'})\n",
    "\n",
    "    # fix GEOID type\n",
    "MEDSL_inspect['GEOID-5'] = MEDSL_inspect['GEOID-5'].astype(str)\n",
    "\n",
    "    # Confirm\n",
    "#print(MEDSL_inspect.info())\n",
    "#print(MEDSL_inspect['office'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f5ae6da-c471-4027-8073-665b6ea95549",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Keep single race to avoid counting multiple votes per person\n",
    "MEDSL_clean = MEDSL_inspect[MEDSL_inspect['office'] == 'AUDITOR GENERAL'] \n",
    "MEDSL_clean = MEDSL_clean.copy().drop(columns=['office']) # closest vote total to VTD\n",
    "\n",
    "    # Fill any missing values and change GREEN and LIBERTARIAN to 'OTHER' \n",
    "MEDSL_clean.loc[:, 'PARTY'] = MEDSL_clean['PARTY'].fillna('OTHER')\n",
    "MEDSL_clean['PARTY'] = MEDSL_clean['PARTY'].replace({'GREEN': 'OTHER', \n",
    "                                                     'LIBERTARIAN': 'OTHER'})\n",
    "    # Confirm\n",
    "#print(MEDSL_clean.info())\n",
    "#print(MEDSL_clean['PARTY'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac35fb18-7747-4020-a4e5-3624570b2f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MEDSL_tranfm2 = MEDSL_transform1.copy().drop(columns=['precinct'])\n",
    "\n",
    "    # Group duplicate precinct, sum vote totals\n",
    "MEDSL_tranfm2 = (MEDSL_tranfm2\n",
    "    .groupby(['GEOID-5', 'PRECINCT'], as_index=False).sum())\n",
    "\n",
    "    # Confirm\n",
    "#print(MEDSL_tranfm2.info())\n",
    "#print(MEDSL_tranfm2.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa008b47-ba76-4822-ad9a-c3e7bc73b265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67 entries, 0 to 66\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   GEOID-5  67 non-null     object\n",
      " 1   PARTY    67 non-null     object\n",
      " 2   VOTES    67 non-null     int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(MEDSL_tranfm2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f3bb4e7-fbae-403b-af0a-1dc74dc75f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['DEM_VOTES', 'REP_VOTES', 'OTH_VOTES'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Change vote columns to int32\u001b[39;00m\n\u001b[0;32m      8\u001b[0m vote_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEM_VOTES\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREP_VOTES\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOTH_VOTES\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m MEDSL_tranfm3[vote_cols] \u001b[38;5;241m=\u001b[39m \u001b[43mMEDSL_tranfm3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvote_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['DEM_VOTES', 'REP_VOTES', 'OTH_VOTES'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "    # Rename columns\n",
    "MEDSL_tranfm3 = MEDSL_tranfm2.rename(columns={\n",
    "    'DEMOCRAT': 'DEM_VOTES',\n",
    "    'REPUBLICAN': 'REP_VOTES',\n",
    "    'OTHER': 'OTH_VOTES'})\n",
    "\n",
    "    # Change vote columns to int32\n",
    "vote_cols = ['DEM_VOTES', 'REP_VOTES', 'OTH_VOTES']\n",
    "MEDSL_tranfm3[vote_cols] = MEDSL_tranfm3[vote_cols].astype('int32')\n",
    "\n",
    "    # Confirm\n",
    "#print(MEDSL_tranfm3.head())\n",
    "#print(MEDSL_tranfm3.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb69cf-c68d-4197-86a0-4d0cf787fb99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MEDSL_tranfm4 = MEDSL_tranfm3.copy()\n",
    "\n",
    "    # Compute TOTAL_VOTES, drop any where the sum of all votes = 0\n",
    "MEDSL_tranfm4['TOTAL_VOTES'] = MEDSL_tranfm4[vote_cols].sum(axis=1).astype('int32')\n",
    "MEDSL_tranfm4 = MEDSL_tranfm4[MEDSL_tranfm4['TOTAL_VOTES'] != 0]\n",
    "\n",
    "    # Compute shares of votes\n",
    "MEDSL_tranfm4['DEM_SHARE'] = ((MEDSL_tranfm4['DEM_VOTES'] / MEDSL_tranfm4['TOTAL_VOTES'])* 100).round(2)\n",
    "MEDSL_tranfm4['REP_SHARE'] = ((MEDSL_tranfm4['REP_VOTES'] / MEDSL_tranfm4['TOTAL_VOTES'])* 100).round(2)\n",
    "MEDSL_tranfm4['OTH_SHARE'] = ((MEDSL_tranfm4['OTH_VOTES'] / MEDSL_tranfm4['TOTAL_VOTES'])* 100).round(2)\n",
    "\n",
    "    # Confirm\n",
    "#print(MEDSL_tranfm4.head())\n",
    "#print(MEDSL_tranfm4.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861238c1-9836-410e-af43-2b6a918f9788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MEDSL_tranfm5 = MEDSL_tranfm4.copy()\n",
    "\n",
    "    # Define the political leaning function\n",
    "def determine_win(row):\n",
    "    shares = {\n",
    "        'DEM': row['DEM_SHARE'],\n",
    "        'REP': row['REP_SHARE'],\n",
    "        'OTH': row['OTH_SHARE']} \n",
    "      # Sort shares in descending order\n",
    "    sorted_shares = sorted(shares.items(), key=lambda x: x[1], reverse=True)\n",
    "      # Top party\n",
    "    max_party, max_value = sorted_shares[0]  \n",
    "    second_party, second_value = sorted_shares[1]\n",
    "      # Difference between top two parties\n",
    "    diff = round(max_value - second_value, 2)\n",
    "      # Determine PARTY_WIN (Binary: DEM = 1, otherwise 0)\n",
    "    party_win = 1 if max_party == 'DEM' else 0\n",
    "    return party_win, diff\n",
    "\n",
    "    # Apply function and create two new variables\n",
    "MEDSL_tranfm5[['PARTY_WIN', 'PARTY_LEAD']] = MEDSL_tranfm5.apply(determine_win, axis=1).apply(pd.Series)\n",
    "\n",
    "# Confirm output\n",
    "print(MEDSL_tranfm5[['PARTY_WIN', 'PARTY_LEAD']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83768481-ad9d-44d3-b4f4-46a4b3768ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Order variables\n",
    "final_cols = ['GEOID-5', 'PRECINCT', 'TOTAL_VOTES', 'DEM_VOTES', 'DEM_SHARE', \n",
    "              'REP_VOTES', 'REP_SHARE', 'OTH_VOTES', 'OTH_SHARE', 'PARTY_LEAD', 'PARTY_WIN']\n",
    "MEDSL_tidy = MEDSL_tranfm5[final_cols]\n",
    "\n",
    "    # Confirm\n",
    "#print(MEDSL_tidy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d8ba9-e2a7-4829-80ad-b15b87a64ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEDSL_tidy.to_excel('MEDSL_tidy.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "16d0ff77-6488-46eb-810d-97bf12225536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "    # Import here for code debugging the merge, if required\n",
    "\n",
    "VTD_tidy = pd.read_excel('VTD_tidy.xlsx')\n",
    "DHC_tidy = pd.read_excel('DHC_tidy.xlsx')\n",
    "MEDSL_tidy = pd.read_excel('MEDSL_tidy.xlsx')\n",
    "\n",
    "# inspection codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e8674e04-4069-4325-94f1-39e941a52e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reg_other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'reg_other'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m MID_tidy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(DHC_tidy, VTD_tidy, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEOID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Combine other and unknown voters\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m MID_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_oth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mMID_tidy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreg_other\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m MID_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_unk\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m MID_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoted_oth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MID_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoted_other\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m MID_tidy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoted_unk\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m MID_tidy\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_unk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoted_unk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturnout_unk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturnout_gap_unk\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_other\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoted_other\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ADV_analytics\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'reg_other'"
     ]
    }
   ],
   "source": [
    "    # Merge first two files\n",
    "MID_tidy = pd.merge(DHC_tidy, VTD_tidy, on='GEOID', how='outer')\n",
    "\n",
    "    # Combine other and unknown voters\n",
    "MID_tidy['reg_oth'] = MID_tidy['reg_other'] + MID_tidy['reg_unk']\n",
    "MID_tidy['voted_oth'] = MID_tidy['voted_other'] + MID_tidy['voted_unk']\n",
    "MID_tidy.drop(columns=['reg_unk', 'voted_unk', 'turnout_unk', 'turnout_gap_unk', \n",
    "                          'reg_other', 'voted_other'], inplace=True)\n",
    "\n",
    "    # Need to adjust GEOID in order to merge with MEDSL\n",
    "MID_tidy['GEOID-10'] = MID_tidy['GEOID'].astype(str).str[:-1].astype(str)\n",
    "\n",
    "    # Confirm\n",
    "print(MID_tidy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd544d-b82b-4c98-9bbf-ed2add5bc13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Merge with third dataset\n",
    "PA_VOTE_DF = pd.merge(MID_join, MEDSL_join, on='JOIN', how='outer')\n",
    "\n",
    "    # Drop columns after merging\n",
    "PA_VOTE_DF.drop(columns=[\n",
    "    'GEOID', 'GEOID-10', 'GEOID-5_x', 'GEOID-5_y', \n",
    "    'PRECINCT', 'BIN_x', 'BIN_y'], inplace=True)\n",
    "\n",
    "    # Confirm\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(PA_VOTE_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1576936-55a2-4938-a2b9-fb9557b17d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Set PARTY_WIN values to 0, will rerun function to clear merging errors\n",
    "PA_VOTE_DF['PARTY_WIN'] = 0\n",
    "\n",
    "    # group together variables for sum or mean\n",
    "PA_VOTE_SUM = ['TOT_POP', 'URBAN', 'RURAL', 'F_U18', 'M_U18', 'F_18-20s', 'M_18-20s', \n",
    "               'F_30s', 'M_30s', 'F_40s', 'M_40s', 'F_50s', 'M_50s', 'F_60up', 'M_60up', \n",
    "               'MALE', 'FEMALE', 'F_ALONE', 'M_ALONE', 'F_N_ALONE', 'M_N_ALONE', \n",
    "               'O_SEX_REL', 'S_SEX_REL', 'OTHER_HH', 'reg_all', 'voted_all', \n",
    "               'reg_eur', 'voted_eur', 'reg_hisp', 'voted_hisp', 'reg_aa', 'voted_aa', \n",
    "               'reg_asia', 'voted_asia', 'reg_oth', 'voted_oth',  \n",
    "               'DEM_VOTES', 'REP_VOTES', 'OTH_VOTES', 'TOTAL_VOTES', 'PARTY_WIN']\n",
    "\n",
    "PA_VOTE_MEAN = ['URBAN_RATIO', 'RURAL_RATIO', 'MED_AGE', 'MED_AGE_M', 'MED_AGE_F', \n",
    "                'F_U18_RATIO', 'M_U18_RATIO', 'F_18-20s_RATIO', 'M_18-20s_RATIO', \n",
    "                'F_30s_RATIO', 'M_30s_RATIO', 'F_40s_RATIO', 'M_40s_RATIO', \n",
    "                'F_50s_RATIO', 'M_50s_RATIO', 'F_60up_RATIO', 'M_60up_RATIO', \n",
    "                'MALE_RATIO', 'FEMALE_RATIO', 'F_SINGLE_RATIO', 'M_SINGLE_RATIO', \n",
    "                'F_LED_HH_RATIO', 'M_LED_HH_RATIO', 'HH_IN_HH', 'OPPSEX_RATIO', 'SAMESEX_RATIO', \n",
    "                'OTH_HH_RATIO', 'turnout_all', 'turnout_eur', 'turnout_gap_eur', \n",
    "                'turnout_hisp', 'turnout_gap_hisp', 'turnout_aa', 'turnout_gap_aa', \n",
    "                'turnout_asia', 'turnout_gap_asia', 'turnout_oth', 'turnout_gap_oth', \n",
    "                'DEM_SHARE', 'REP_SHARE', 'OTH_SHARE', 'PARTY_LEAD']\n",
    "\n",
    "    # Group by JOIN and aggregate\n",
    "PA_VOTE_A = PA_VOTE_DF.groupby('JOIN').agg({\n",
    "    **{col: 'sum' for col in PA_VOTE_SUM if col in PA_VOTE_DF.columns}, \n",
    "    **{col: 'mean' for col in PA_VOTE_MEAN if col in PA_VOTE_DF.columns}\n",
    "      }).reset_index()\n",
    "\n",
    "    # Convert summed variables to int\n",
    "PA_VOTE_A[PA_VOTE_SUM] = PA_VOTE_A[PA_VOTE_SUM].astype('int32')\n",
    "PA_VOTE_A[PA_VOTE_MEAN] = PA_VOTE_A[PA_VOTE_MEAN].astype('float32').round(2)\n",
    "\n",
    "    # Confirm\n",
    "print(PA_VOTE_A.info())\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#print(PA_VOTE_A.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6416223-79d2-4eea-8253-104ed250f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_VOTE_B = pd.read_excel('PA_VOTE_B.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793eecc5-7298-4e53-aeb6-43a43592d23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Drop any records where TOTAL_VOTES are 0\n",
    "#PA_VOTE_B = PA_VOTE_A[PA_VOTE_A['TOTAL_VOTES'] != 0].copy()\n",
    "\n",
    "    # Identify columns that contain 'turnout_'\n",
    "#columns_to_zeroize = [col for col in PA_VOTE_B.columns if 'turnout_' in col]\n",
    "\n",
    "   # Clear all to recalculate ratios and turnouts\n",
    "#PA_VOTE_B[columns_to_zeroize] = 0\n",
    "\n",
    "print(PA_VOTE_B.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f5b3c-2324-4e1c-b309-52cc4a006a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Compute turnout as voted_ / reg_ for each column\n",
    "turnout_cols = ['all', 'eur', 'hisp', 'aa', 'asia', 'oth']\n",
    "for col in turnout_cols:\n",
    "   voted_col = f'voted_{col}'\n",
    "   reg_col = f'reg_{col}'\n",
    "   turnout_col = f'turnout_{col}'\n",
    "\n",
    "     # Calculate turnout only where registration is greater than zero to avoid division errors\n",
    "   PA_VOTE_B[turnout_col] = PA_VOTE_B[voted_col] / PA_VOTE_B[reg_col]\n",
    "     # Replace NaN with 0 where division is undefined\n",
    "   PA_VOTE_B[turnout_col] = PA_VOTE_B[turnout_col].fillna(0)  \n",
    "\n",
    "   # recalculate \"turnout_gap\" for each group (turnout_grp - turnout_all)\n",
    "for grp in turnout_cols:\n",
    "   col_name = f'turnout_{grp}'\n",
    "   PA_VOTE_B[col_name] = (PA_VOTE_B[col_name] * 100).round(2)\n",
    "\n",
    "for grp in turnout_cols[1:]:\n",
    "   PA_VOTE_B[f'turnout_gap_{grp}'\n",
    "    ] = PA_VOTE_B[f'turnout_{grp}'] - PA_VOTE_B['turnout_all']\n",
    "\n",
    "    # Confirm\n",
    "print(PA_VOTE_B.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecc899-b53c-4e83-a018-adf56cdef2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_VOTE_C = pd.read_excel('PA_VOTE_B.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f05e0cdd-c79d-4bbb-b158-3acc81dbf310",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PA_VOTE_B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Round all ratio (float) columns to 2 decimal places\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m PA_VOTE_C \u001b[38;5;241m=\u001b[39m \u001b[43mPA_VOTE_B\u001b[49m\u001b[38;5;241m.\u001b[39mround({col: \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m PA_VOTE_B\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns})\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set JOIN as index, and drop it from numerical calculations\u001b[39;00m\n\u001b[0;32m      5\u001b[0m PA_VOTE_C \u001b[38;5;241m=\u001b[39m PA_VOTE_C\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOIN\u001b[39m\u001b[38;5;124m'\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PA_VOTE_B' is not defined"
     ]
    }
   ],
   "source": [
    "    # Round all ratio (float) columns to 2 decimal places\n",
    "PA_VOTE_C = PA_VOTE_B.round({col: 2 for col in PA_VOTE_B.select_dtypes(include=['float']).columns}).copy()\n",
    "\n",
    "    # Set JOIN as index, and drop it from numerical calculations\n",
    "PA_VOTE_C = PA_VOTE_C.set_index('JOIN', drop=True)\n",
    "\n",
    "    # Apply function again\n",
    "PA_VOTE_C['PARTY_WIN'] = PA_VOTE_C.apply(lambda row: determine_win(row)[0], axis=1)\n",
    "\n",
    "    # EDA - Correlation Heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(PA_VOTE_C.corr(), cmap='coolwarm', annot=False, linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "    # EDA - Distribution Check\n",
    "PA_VOTE_C.hist(figsize=(12, 8), bins=30, edgecolor='black')\n",
    "plt.suptitle('Feature Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea94d4-da33-4b7b-b282-f062147ceee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Identify highly correlated features (threshold = 0.85)\n",
    "corr_matrix = PA_VOTE_C.corr().abs()\n",
    "upper_tri = corr_matrix.where(\n",
    "    ~corr_matrix.map(lambda x: x == 1).astype(bool)\n",
    ").where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Features to drop due to high correlation\n",
    "high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
    "\n",
    "    # Compute feature variances, drop very low variance\n",
    "variances = PA_VOTE_C.var()\n",
    "low_variance = variances[variances < 0.01].index.tolist()\n",
    "\n",
    "    # Combine both feature sets to drop\n",
    "features_to_drop = list(set(high_corr + low_variance))\n",
    "\n",
    "    # Output list of features to drop\n",
    "print(features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818a719-ff18-44b6-a908-9186693f87fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Create a copy of the dataset for clustering\n",
    "PA_VOTE_cluster = PA_VOTE_C.copy()\n",
    "\n",
    "    # Apply transformations (inverting selected variables)\n",
    "PA_VOTE_cluster['turnout_oth_inv'] = PA_VOTE_cluster['turnout_oth'] * -1\n",
    "PA_VOTE_cluster['F_SINGLE_RATIO_inv'] = PA_VOTE_cluster['F_SINGLE_RATIO'] * -1\n",
    "PA_VOTE_cluster['M_SINGLE_RATIO_inv'] = PA_VOTE_cluster['M_SINGLE_RATIO'] * -1\n",
    "PA_VOTE_cluster['F_LED_HH_RATIO_inv'] = PA_VOTE_cluster['F_LED_HH_RATIO'] * -1\n",
    "\n",
    "    # Define the final selected features for clustering\n",
    "selected_vars = ['URBAN_RATIO', 'turnout_hisp', 'turnout_oth_inv', 'F_30s_RATIO', 'F_50s_RATIO', \n",
    "                 'SAMESEX_RATIO', 'F_SINGLE_RATIO_inv', 'M_SINGLE_RATIO_inv', 'F_LED_HH_RATIO_inv']\n",
    "\n",
    "    # Standardize the selected features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(PA_VOTE_cluster[selected_vars])\n",
    "\n",
    "    # Run K-Means clustering\n",
    "kmeans_13 = KMeans(n_clusters=13, random_state=630, n_init=10)\n",
    "\n",
    "    # Set\n",
    "PA_VOTE_cluster[selected_vars + ['PARTY_WIN']]\n",
    "\n",
    "PA_VOTE_cluster['Cluster_13'] = kmeans_13.fit_predict(scaled_data)\n",
    "\n",
    "    # Inspect results\n",
    "print(PA_VOTE_cluster.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce7a3a-e709-4020-add6-16c993c69f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete, substitute code\n",
    "selected_vars = [col for col in PA_VOTE_cluster.columns if col != 'PARTY_WIN']\n",
    "party_win_column = PA_VOTE_cluster['PARTY_WIN']\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(PA_VOTE_cluster[selected_vars])\n",
    "kmeans_13 = KMeans(n_clusters=13, random_state=630, n_init=10)\n",
    "PA_VOTE_cluster['Cluster_13'] = kmeans_13.fit_predict(scaled_data)\n",
    "PA_VOTE_cluster['PARTY_WIN'] = party_win_column\n",
    "print(PA_VOTE_cluster.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc5221-e112-4418-8fd4-bd03d5c86b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Compute distribution of PARTY_WIN across clusters\n",
    "cluster_distro = (PA_VOTE_cluster.groupby('Cluster_13')['PARTY_WIN']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack() * 100)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "cluster_distro.rename(columns={0: 'Republican (%)', 1: 'Democrat (%)'}, inplace=True)\n",
    "\n",
    "    # Fill missing values with 0 a few will be 0\n",
    "cluster_distro = cluster_distro.fillna(0)\n",
    "\n",
    "    # Set df\n",
    "PA_VOTE_13 = cluster_distro.copy()\n",
    "\n",
    "    # Confirm\n",
    "print(PA_VOTE_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c37ca-7d10-414d-8b69-13b5a8831f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Extract cluster centers\n",
    "cluster_centers = kmeans_13.cluster_centers_\n",
    "\n",
    "    # Create a new df for better visualization\n",
    "cluster_centers_df = pd.DataFrame(cluster_centers, columns=selected_vars)\n",
    "\n",
    "    # Plot the cluster centers\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(len(cluster_centers)):\n",
    "   plt.plot(cluster_centers_df.iloc[i], label=f'Cluster {i}')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Cluster Center Value\")\n",
    "plt.title(\"K-Means Cluster Centers for Each Feature\")\n",
    "plt.legend(title=\"Clusters\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "    # Create Cluster Heatmap\n",
    "plt.figure(figsize=(15, 40))\n",
    "sns.clustermap(pd.DataFrame(scaled_data, columns=selected_vars),\n",
    "               method='average',\n",
    "               metric='euclidean',\n",
    "               figsize=(12, 12),\n",
    "               cmap=\"coolwarm\",\n",
    "               col_cluster=False)  # Cluster only on rows\n",
    "\n",
    "plt.title(\"Cluster Heatmap of Scaled Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe223be-f594-484e-a76a-1e7b7e193074",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create pairplot with PARTY_WIN hue\n",
    "sns.pairplot(PA_VOTE_cluster, hue='PARTY_WIN', palette={0: 'red', 1: 'blue'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309f58e-1ec1-4406-baa3-2407de0d2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select significant predictors for logistic regression\n",
    "\n",
    "initial_vars = ['URBAN_RATIO', 'turnout_hisp', 'turnout_oth_inv', 'F_30s_RATIO', 'F_50s_RATIO', \n",
    "                'SAMESEX_RATIO', 'F_SINGLE_RATIO_inv', 'M_SINGLE_RATIO_inv', 'F_LED_HH_RATIO_inv']\n",
    "PA_VOTE_1 = PA_VOTE_cluster[initial_vars]\n",
    "y = PA_VOTE_cluster['PARTY_WIN']\n",
    "\n",
    "# Standardize selected features\n",
    "scaler = StandardScaler()\n",
    "trim_1 = scaler.fit_transform(PA_VOTE_1)\n",
    "\n",
    "# Add constant for intercept\n",
    "trim_1 = sm.add_constant(trim_1)\n",
    "\n",
    "# Fit logistic regression model with selected features\n",
    "logit_1_model = sm.Logit(y, trim_1).fit()\n",
    "\n",
    "# Display results\n",
    "logit_1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f606e85-e60b-47f3-9fec-497c992e8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Pseudo R = 0.1760 (Moderate explanatory power)\n",
    "    Log-Likelihood = -434.31 (Improved from null model: -527.07)\n",
    "    LLR p-value = 3.587e-35 (Highly significant overall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762dabd8-cd6e-4d43-9082-fb5c8cb500c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select significant predictors for logistic regression\n",
    "\n",
    "final_vars = ['URBAN_RATIO', 'turnout_hisp', 'F_30s_RATIO', 'SAMESEX_RATIO', 'F_LED_HH_RATIO_inv']\n",
    "PA_VOTE_2 = PA_VOTE_cluster[final_vars]\n",
    "y = PA_VOTE_cluster['PARTY_WIN']\n",
    "\n",
    "# Standardize selected features\n",
    "scaler = StandardScaler()\n",
    "trim_2 = scaler.fit_transform(PA_VOTE_2)\n",
    "\n",
    "# Add constant for intercept\n",
    "trim_2 = sm.add_constant(trim_2)\n",
    "\n",
    "# Fit logistic regression model with selected features\n",
    "logit_2_model = sm.Logit(y, trim_2).fit()\n",
    "\n",
    "# Display results\n",
    "logit_2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80d8cd-73c6-4a1b-a688-803755470e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo R = 0.1750 (Slight decrease from 0.1760, but still strong)  \n",
    "Log-Likelihood = -434.81 (Minimal change from -434.31)  \n",
    "LLR p-value = 3.729e-37 (Highly significant, now e-02 smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f74cd0-e5b1-4af8-8c96-c499cde0ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predicted probabilities using the updated logit_2_model\n",
    "pred_2_probs = logit_2_model.predict(trim_2)  # Probabilities\n",
    "pred_2 = (pred_2_probs > 0.5).astype(int)  # Convert to binary predictions\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_2 = accuracy_score(y, pred_2)\n",
    "report_2 = classification_report(y, pred_2)\n",
    "conf_matrix_2 = confusion_matrix(y, pred_2)\n",
    "\n",
    "# Display results\n",
    "print(f'Logistic Regression Accuracy: {accuracy_2:.4f}')\n",
    "print(report_2)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640e7b9-f562-4878-b122-eec04397b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "coefs = logit_2_model.params[1:]  # Exclude the constant term\n",
    "features = final_vars  # Use original feature names\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(final_vars, coefs)\n",
    "plt.xlabel('Logistic Regression Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance in Predicting PARTY_WIN')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')  # Reference line at 0\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d49f956-df81-4641-a9b2-b4af341b825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "95e11606-aa1e-4c61-ae62-e5c7b72be653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = Document()\n",
    "\n",
    "# Title Page\n",
    "doc.add_heading(\"Predicting Political Outcomes with Clustering & Logistic Regression\", level=1)\n",
    "doc.add_paragraph(\"ANA 630 - Final Project\\nJason Noble\\nDate\")\n",
    "\n",
    "# Abstract\n",
    "doc.add_heading(\"Abstract\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"This study explores whether demographic characteristics can reliably predict party affiliation \"\n",
    "    \"in Pennsylvanias 2020 election. Using clustering (K-Means, Hierarchical Agglomerative Clustering) \"\n",
    "    \"and logistic regression, we identify key predictors and evaluate their statistical significance. \"\n",
    "    \"Feature selection, hypothesis testing, and model performance were analyzed to determine which variables \"\n",
    "    \"had the most impact. The final logistic regression model achieved **79.5% accuracy**, confirming that factors \"\n",
    "    \"such as urbanization, household structure, and voter turnout ratios play a role in predicting political outcomes.\"\n",
    ")\n",
    "\n",
    "# Introduction\n",
    "doc.add_heading(\"Introduction\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"Objective:\\n\"\n",
    "    \"- Null Hypothesis (H): No demographics can predict party affiliation.\\n\"\n",
    "    \"- Alternative Hypothesis (H): There are specific demographic characteristics that can reliably predict party affiliation.\\n\"\n",
    "    \"- Compare different clustering strategies to identify **optimal k**.\\n\"\n",
    "    \"- Predict which party (Republican or Democrat) will win a region using clustering and logistic regression.\\n\"\n",
    "    \"- Assess model accuracy and predictive power.\"\n",
    ")\n",
    "\n",
    "# The Challenge of the Merge\n",
    "doc.add_heading(\"The Challenge of the Merge\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"Each voting district begins with a **GEOID** in their 20-digit ID, so initially, joining datasets using GEOID seemed \"\n",
    "    \"like a viable approach. However, discrepancies arose between datasets:\\n\"\n",
    "    \"- **MEDSL vote counts differ from VTD** (MEDSL is raw precinct votes before county verification).\\n\"\n",
    "    \"- **Some counties use voting precincts as Census districts, while others do not.**\\n\"\n",
    "    \"- **Without Secretary of State info, clean conversion was impossible.**\\n\\n\"\n",
    "    \"Approach taken:\\n\"\n",
    "    \"- **Convert GEOID structures** (VTD.GEOID-12 and DHC.GEOID-15) to **GEOID-11**.\\n\"\n",
    "    \"- **Aggregate data** using sum or mean during merging.\\n\"\n",
    "    \"- **Assign bin labels** to categorize precincts for merging.\\n\"\n",
    "    \"- **Recalculate turnout ratios after merging.**\"\n",
    ")\n",
    "\n",
    "# The Data\n",
    "doc.add_heading(\"The Data\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Datasets Used:**\\n\"\n",
    "    \"1. **VTD:** L2 Voter File - 2020 Elections Turnout Statistics for Pennsylvania, aggregated to VTD level (GEOID-11).\\n\"\n",
    "    \"   - 149 variables x 336,986 records.\\n\"\n",
    "    \"2. **DHC:** U.S. Census Bureau - 2020 Select Demographic and Housing Characteristic (DHC) Data for Pennsylvania.\\n\"\n",
    "    \"   - 90 variables x 3,447 records.\\n\"\n",
    "    \"3. **MEDSL:** MIT Election Data and Science Lab - 2020 general election results for Pennsylvania, at the precinct level.\\n\"\n",
    "    \"   - 25 variables x 186,492 records.\"\n",
    ")\n",
    "\n",
    "# Data Preprocessing\n",
    "doc.add_heading(\"Data Preprocessing\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Challenges & Efforts:**\\n\"\n",
    "    \"- MEDSL data was **filtered for November 2020 elections** and matched to one race to ensure one vote per person.\\n\"\n",
    "    \"- Missing **PARTY data was corrected** (e.g., one Trump vote labeled incorrectly as 'Other').\\n\"\n",
    "    \"- **80 join groups lacked demographic data** post-merge but still had vote tallies  These were redistributed proportionally.\\n\"\n",
    "    \"- **Feature engineering:**\\n\"\n",
    "    \"  - Combined variables into logical groups (age, housing, turnout categories).\\n\"\n",
    "    \"  - Normalized distributions where necessary.\\n\"\n",
    "    \"  - Dependent Variable: **PARTY_WIN (Republican = 0, Democrat = 1).**\"\n",
    ")\n",
    "\n",
    "# Clustering Approach\n",
    "doc.add_heading(\"Clustering Approach\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Methods Tested:**\\n\"\n",
    "    \"- **K-Means Clustering** and **Hierarchical Agglomerative Clustering (HAC).**\\n\"\n",
    "    \"- **Calculated WCSS** to determine optimal k (Elbow Method ~13).\\n\\n\"\n",
    "    \"**Findings:**\\n\"\n",
    "    \"- **13 Clusters:** Showed clear voting patterns, but were strongly Republican.\\n\"\n",
    "    \"- **68 Clusters:** More balanced PARTY_WIN distribution, but worse overall model performance.\\n\"\n",
    "    \"- Standardized features made HAC **less effective** (variables became too uniform).\"\n",
    ")\n",
    "\n",
    "# Cluster Distributions\n",
    "doc.add_heading(\"Cluster Distributions\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Key Observations:**\\n\"\n",
    "    \"- **13 Clusters:** Certain clusters were **overwhelmingly Republican (>25% higher share).**\\n\"\n",
    "    \"- **Two clusters were competitive (50/50 split).**\\n\"\n",
    "    \"- **68 Clusters caused over-segmentation**, limiting pattern clarity.\"\n",
    ")\n",
    "\n",
    "# Logistic Regression Approach\n",
    "doc.add_heading(\"Logistic Regression Approach\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Why Logistic Regression?**\\n\"\n",
    "    \"- Ideal for **binary outcomes** (Republican = 0, Democrat = 1).\\n\"\n",
    "    \"- Allows for **feature selection and model interpretability.**\\n\\n\"\n",
    "    \"**Feature Selection Process:**\\n\"\n",
    "    \"1. **Started with full feature set** (high collinearity issues).\\n\"\n",
    "    \"2. **Removed redundant features** using Variance Inflation Factor (VIF).\\n\"\n",
    "    \"3. **Kept only statistically significant predictors (p-values < 0.05).**\"\n",
    ")\n",
    "\n",
    "# Model Performance & Accuracy\n",
    "doc.add_heading(\"Model Performance & Accuracy\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Final Model Performance (13 Clusters):**\\n\"\n",
    "    \"- **Accuracy: ~79.5%.**\\n\"\n",
    "    \"- **High precision for Republicans (81%).**\\n\"\n",
    "    \"- **Low recall for Democrat wins (27%)  Many misclassified as Republican.**\\n\\n\"\n",
    "    \"**Confusion Matrix:**\\n\"\n",
    "    \"| Actual vs. Predicted | Predicted Republican (0) | Predicted Democrat (1) |\\n\"\n",
    "    \"|----------------------|-------------------------|------------------------|\\n\"\n",
    "    \"| **Actual Republican (0)** | **704** (True Negatives) | **31** (False Positives) |\\n\"\n",
    "    \"| **Actual Democrat (1)** | **166** (False Negatives) | **62** (True Positives) |\\n\"\n",
    ")\n",
    "\n",
    "# Key Findings\n",
    "doc.add_heading(\"Key Findings\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Summary of Insights:**\\n\"\n",
    "    \"- **Clustering revealed distinct political patterns.**\\n\"\n",
    "    \"- **Logistic regression successfully predicted Republican wins but struggled with Democrat classification.**\\n\"\n",
    "    \"- **Feature selection improved model performance.**\\n\\n\"\n",
    "    \"**Limitations & Next Steps:**\\n\"\n",
    "    \"- **Dataset imbalance** (more Republican wins than Democrats).\\n\"\n",
    "    \"- The merging process **spread Democratic votes, diluting their impact.**\\n\"\n",
    "    \"- Consider **alternative dataset joining methods.**\\n\"\n",
    "    \"- Explore **Random Forest, SVM, or ensemble models** for better classification.\\n\"\n",
    "    \"- Continue **feature engineering** to improve recall for Democrats.\"\n",
    ")\n",
    "\n",
    "# Conclusion & Recommendations\n",
    "doc.add_heading(\"Conclusion & Recommendations\", level=2)\n",
    "doc.add_paragraph(\n",
    "    \"**Final Takeaways:**\\n\"\n",
    "    \"- **Clustering provided useful insights** into regional voting trends.\\n\"\n",
    "    \"- **Logistic regression worked well but needs improvement in Democrat predictions.**\\n\"\n",
    "    \"- **13 clusters proved to be the best strategy.**\\n\\n\"\n",
    "    \"**Recommendations:**\\n\"\n",
    "    \"- ***Find a way to accurately merge all three datasets.***\\n\"\n",
    "    \"- **Use resampling techniques (SMOTE) to balance dataset.**\\n\"\n",
    "    \"- **Try alternative classification models for better recall.**\\n\"\n",
    "    \"- **Refine feature selection with PCA.**\"\n",
    ")\n",
    "\n",
    "    # Save\n",
    "docx_file_path = r\"C:/Users/nobul/anaconda3/ALL_DATA/ANA630/NOBLE_ANA630_Final.docx\"\n",
    "doc.save(docx_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216b744-4ee6-4046-8e10-0a596c9b716c",
   "metadata": {},
   "source": [
    "# Compute VIF for housing variables\n",
    "X_housing = PA_VOTE_cluster[['F_SINGLE_RATIO_inv', 'M_SINGLE_RATIO_inv', \n",
    "                             'F_LED_HH_RATIO_inv', 'M_LED_HH_RATIO_inv']]\n",
    "X_housing = sm.add_constant(X_housing)  # Add constant for proper VIF calculation\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X_housing.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_housing.values, i) for i in range(X_housing.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c73086-9a0f-4d0d-999e-95056f592ca2",
   "metadata": {},
   "source": [
    "Some domains naturally have low intrinsic dimensionality (like voter demographics).If the dataset has inherently low dimensionality (e.g., strong correlations between features), then only a few PCs should matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18259f-9da2-4af0-af6b-d94020f99668",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Check significance of individual features using ANOVA\n",
    "anova_results = {}\n",
    "for feature in manova_features:\n",
    "    model = ols(f\"{feature} ~ HAC_Cluster\", data=PA_VOTE_F).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    anova_results[feature] = anova_table[\"PR(>F)\"].iloc[0]  # Extract p-value\n",
    "\n",
    "# Sort features by significance (lowest p-values first)\n",
    "sorted_features = sorted(anova_results.items(), key=lambda x: x[1])\n",
    "\n",
    "# Select the top 3 most significant features\n",
    "top_3_features = [sorted_features[i][0] for i in range(3)]\n",
    "print(\"Top 3 Features Selected:\", top_3_features)\n",
    "#Top 5 Features Selected: ['voted_all', 'OTH_VOTES', 'M_SINGLE_RATIO', 'F_SINGLE_RATIO', 'M_LED_HH_RATIO']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7b20f79-cb7d-44b1-9d88-694272e808ad",
   "metadata": {},
   "source": [
    "    # Refine remaining features in DF\n",
    "manova_features = [col for col in PA_VOTE_F.columns if col not in ['HAC_Cluster', 'PARTY_WIN']]\n",
    "\n",
    "    # Run MANOVA \n",
    "manova_model = MANOVA.from_formula(\" + \".join(manova_features) + \" ~ HAC_Cluster\", data=PA_VOTE_F)\n",
    "print(manova_model.mv_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5ad0d-8c46-4208-a5c2-63171df7676a",
   "metadata": {},
   "source": [
    "\n",
    "# Standardize the data for K-Means\n",
    "X_kmeans = StandardScaler().fit_transform(PA_VOTE_C.drop(columns=['PARTY_WIN']))\n",
    "\n",
    "# Compute WCSS for a range of clusters\n",
    "wcss = []\n",
    "k_range = range(1, 68)  # Testing k from 1 to 14\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=630, n_init=10)\n",
    "    kmeans.fit(X_kmeans)\n",
    "    wcss.append(kmeans.inertia_)  # Inertia = WCSS\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, wcss, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xticks(k_range)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186c545-cf2c-4dad-ab8e-4dbdc2913cf9",
   "metadata": {},
   "source": [
    "Clustering may not work for this dataset, with such popular cities, it does take a lot of rural areas to equal a single record from Philly or Pittsburg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c530d2f-4470-443a-ad49-5ca56ec64cdd",
   "metadata": {},
   "source": [
    "# Normalize by total votes to adjust for urban vs. rural imbalance\n",
    "PA_VOTE_E['weighted_turnout'] = PA_VOTE_E['voted_all'] / PA_VOTE_E['OTH_VOTES']\n",
    "\n",
    "# Apply log transformation to compress large variations\n",
    "PA_VOTE_E['log_voted_all'] = np.log1p(PA_VOTE_E['voted_all'])\n",
    "PA_VOTE_E['log_OTH_VOTES'] = np.log1p(PA_VOTE_E['OTH_VOTES'])\n",
    "\n",
    "# Select features for clustering\n",
    "features_for_clustering = ['log_voted_all', 'log_OTH_VOTES', 'M_SINGLE_RATIO', 'weighted_turnout']\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "PA_VOTE_scaled = scaler.fit_transform(PA_VOTE_E[features_for_clustering])\n",
    "\n",
    "# Compute linkage using Wards method\n",
    "linkage_weighted = linkage(PA_VOTE_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram to determine optimal cutoff\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_weighted, truncate_mode='level', p=10)\n",
    "plt.axhline(y=8, color='r', linestyle='--')  # Adjust cutoff manually\n",
    "plt.title(\"Dendrogram with Distance Cutoff\")\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "# Set optimal cluster number\n",
    "optimal_k = 5  # Adjust based on dendrogram\n",
    "clusters_weighted = fcluster(linkage_weighted, optimal_k, criterion='maxclust')\n",
    "\n",
    "# Add clusters to DataFrame\n",
    "PA_VOTE_E['HAC_Cluster_Weighted'] = clusters_weighted\n",
    "\n",
    "# Display cluster distribution\n",
    "print(PA_VOTE_E['HAC_Cluster_Weighted'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe599ce-9068-4e8e-9588-b31c4f8dc31b",
   "metadata": {},
   "source": [
    "# Standardize features for hierarchical clustering\n",
    "PA_VOTE_hier = StandardScaler().fit_transform(PA_VOTE_D.drop(columns=['PARTY_WIN']))\n",
    "\n",
    "# Compute linkage matrix\n",
    "linkage_matrix = linkage(PA_VOTE_hier, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Cluster Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef888c-fb8e-4014-9bce-8f72f933328f",
   "metadata": {},
   "source": [
    "    # Apply PCA on the standardized dataset\n",
    "pca = PCA()\n",
    "PA_VOTE_PCA = pca.fit_transform(PA_VOTE_scaled)\n",
    "\n",
    "    # Plot explained variance to determine optimal number of components\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a59d57-d579-488a-aada-e6d7253375bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ADV_analytics)",
   "language": "python",
   "name": "adv_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
